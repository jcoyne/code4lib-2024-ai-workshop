"Think Big, Act Localhost: using large scale data techniques at smaller scales for ad hoc data analysis","Even the largest libraries generally do not work with data that would qualify as Big Data, such as the log data represented by Google searches or Facebook posts. That puts some libraries in a bit of a bind, what we might call a ""Medium Data"" problem. The catalog data of a larger library, measured at the scale of a many millions of records, is interesting enough to be the subject of intra-data set analysis, but large enough that it can exceed the simple programming and scripting techniques used for ad hoc analysis.

This talk will present intermediate level programming techniques for working in this intermediary space between larger infrastructure projects and ad hoc, experimental computer programming. It will discuss strategies for how simple scripts that do not utilize databases or search indices can still process millions of records efficiently. The canonical example used to illustrate these techniques will be a simple FRBRization clustering algorithm for approximately 10 million MARC records. FRBR, or Functional Requirements for Bibliographic Records, specifies a model in which multiple instances of the same work, such as the print, electronic and microform copies of the same title, can be clustered together as a single unit."
Exploring Subject Analysis with Annif&#58; Testing a Machine Learning Tool for Subject Heading Creation,"Machine learning has the potential to improve opportunities for metadata enhancement. This poster will present some of the most interesting findings from my project as a LEADING Fellow at the Metadata Research Center at Drexel University, working with OCLC Research. The goal of the project was to test a new machine learning tool called Annif, created by the National Library of Finland. We wanted to see if it could be used to suggest FAST subject headings for MARC records. Annif is a new tool, still in active development, so these results should be considered preliminary. The most striking results came from testing batches of records based on size. There was a noticeable increase in success based on the size of both the training records and testing records. This could have serious implications for the usefulness of this process on catalog records, which are generally short."
Observability in Library Technology,"Observability allows you to observe what's happening inside of your code and ask questions of it. Within the lens of library technology, observability practices can apply to a variety of systems from library application development to server environments.

Observability requires instrumentation to emit data signals, such as traces, logs, and metrics. That data can then be used to troubleshoot, address issues, transfer knowledge within and across team boundaries, and support stakeholders. Thus, Observability can transform both technical and socio-technical systems.

This poster covers core observability concepts and illustrates use cases within library technology. In highlighting sustainable practices that can improve development speed and reduce toil, the goal is to encourage dialog around creating a better socio-technical development experience."
Designing an open-source application to record Library Door Traffic Counter,"Amid the COVID-19 pandemic, the concept of ""social distancing guidelines"" emerged as a crucial measure to mitigate the spread of the virus. These guidelines are particularly challenging to uphold in high-traffic areas such as university libraries, which experience significant footfall during academic semesters. To address this challenge, library officials have turned to technology, including door traffic count applications, to obtain real-time data on patron capacity. This allows them to effectively monitor and manage social distancing measures within the library premises, ensuring the safety of patrons. Although numerous vendor-based solutions exist to address this requirement, this poster focuses on an open-source application designed to monitor real-time door traffic counts at the Oklahoma State University Library."
Move Fast and Fill Things: Using Python for Full-Text Retrieval,"In 2022, a growing systematic review service at a 5 -person academic health sciences library prompted staff to seek creative ways to manage workload while still providing comprehensive service. Access Services staff, who were novice but enthusiastic coders, developed a Python script to expedite retrieval of licensed and Open Access full-text articles (sometimes numbering in the hundreds) for the review teams.  After incorporating more complex Python libraries and API connections to dramatically reduce retrieval time, staff were inspired to adapt the script for interlibrary loan services. This streamlined lending processes in a way that increased the volume of filled requests while decreasing the overall time commitment. With such a small, versatile team, staff time is paramount, and this opened up opportunities for additional projects and responsibilities. Staff continue to refine the script and adapt it in new ways."
The Compassion of DevOps,"A strong DevOps culture helps teams deliver high quality products on predictable schedules. But in my experience, the strongest DevOps culture is the one that recognizes suffering and offers compassion to a technology workforce that is often over tasked and under resourced. This talk will discuss DevOps best practices, but also how the lack of these practices can take a toll on our bodies and spirits, in ways that are not often acknowledged."
Building a Walking Tour App to Increase Knowledge and Empathy for Complex Historical Sites,"Using a wide range of third party software tools and services we have built a customizable walking tour Android app that integrates Geographic Information Systems and archival content. The first iteration of the app has been demonstrated on the site of District Six in Cape Town, where some 60,000 people of color were forcibly removed and their homes, business, and social institutions were demolished by the apartheid government. The app guides users through the historically erased landscape and allows them to understand what used to exist at specific geolocations through a series of descriptive text and historical visual imagery. Users can even use a “slider” to line up and compare the contemporary landscape with the historical built environment before it was demolished. We feel that the affective impact of the erased landscape and what was lost is rendered more meaningful by guiding users to the exact location where homes, houses of worship, business, and social institutions once stood. The app is currently being piloted by the District Six Museum as a tool to train a new generation of walking tour guides. More broadly, the app is designed to be site independent, meaning any physical landscape / location can be used as the basis for a walking tour and where creators can incorporate their own unique sites, points of interest, maps, content, and stories."
The Distant Reader,"The Distant Reader is a tool for... distant reading. Given an almost arbitrary number of files of just about any type, the Reader applies text mining and natural language processing against the input and outputs a data set amenable to computation. These data sets are designed to stand the test of time, meaning they are operating system and network independent. These data sets are akin to library collections, and examples include: 1) the complete works of Jane Austen, 2) 20,000 articles on the topic of COVID-19, or 3) digitized Catholic pamphlets. The Reader not only creates these data sets, but it also provide the ability to analyze -- model -- them. Some of the out-of-the-box modeling techniques include: 1) frequencies of extracted features (ngrams, parts-of-speech, named entities, computed keywords), topic modeling, semantic indexing, Linked Data, and sentence extraction based on given grammars. More recently, work has been done to apply large-language models to the data sets. Just as importantly, one does not need to use the Reader to do analysis; a great deal of analysis can be done against these data sets using off-the-shelf GUI applications or any computer programming language."
"Case Study, Jr.&#58; How Digital Preservation System Designs Can Impact Workflows","How might an institution work around the limitations of their digital preservation system? As an example, or mini-case study, I will explain how our organization, Rice University, modified our archival information packages to work with our preservation system's ingest process. In this case, we are using Archivematica to properly store our digital archival packages. The system can only ingest packages that are around 30 GB in size. However, we have several collections that are significantly larger. For example, the Houston Asian American Archive's digital collection is over 590 GB. How can we ingest this collection when there is a 30 GB threshold per package? Through much trial and error, we devised a workflow that allows us to split our package into multiple parts and use the preservation system to link the parts to back to each other. With this poster, Rice will happily share how we accomplished this task."
From Screenly to OPACs and more&#58; Putting Idle Raspberry Pis to Work,"The presenter's previous work involved conducting semi-structured interviews with librarians who had utilized maker technologies in their library projects, aiming to gain insight into the potential applications and promote awareness of these possibilities. Subsequently, tutorials and code were created and made publicly available on GitHub, to facilitate the development of similar projects across various library settings. This poster showcases the outcomes of a library case study where these GitHub resources were utilized to implement maker projects within an academic library, aiming to evaluate and enhance the projects for broader adoption."
Aligning Keywords from Long Form Prose to Controlled Vocabulary,"HIVE-4-MAT is a linked-data, automatic indexing application for vocabularies related to material science. In the past few months, work has been done to improve the performance of the keyword alignment algorithm so that it is faster, more accurate, and more flexible at the expense of precision. This presentation reports on the lessons learned in the process of refactoring this keyword alignment algorithm. Since HIVE-4-MAT has a somewhat broad scope, it provides a good use case for analyzing a keyword alignment pipeline from raw article text scraping to keyword extraction to keyword matching and alignment. The presentation will touch topics such as common pitfalls of web scraping, different strategies for preparing raw text for keyword extraction, the differences in goals between keyword extraction and keyword alignment, and the potential benefits and drawbacks of utilizing the concept of string distance in keyword alignment algorithms."
Beyond Band-Aids: Rethinking Accessibility Widgets,"Third-party accessibility widgets are often advertised as a quick fix to make your website accessible. The truth is that no third-party widget can replace a fundamental commitment to designing and maintaining accessible web content… But is there another use case for this kind of tool? We recently launched our twist on the “band-aid” accessibility widget: a set of optional tools to increase perceptibility of our main library website, with a special focus on features for neurodiverse users, such as a dyslexic-friendly font. A conversation with a disability advocacy student group inspired us to build this open-source widget that cedes some control over our site design to the user to improve accessibility, readability, and their overall experience. These features are a step toward ensuring that our diverse population of users has equitable access to our website content. In this talk, we’ll discuss how we used universal design principles to design this widget, give a demonstration, and share user feedback."
"Everything old is new again : What we could have learned from a book on COBOL from 1976, but didn’t","Tech culture so often focuses on the future, it is easy to forget technology has a past. Yet, the long term history of software development, and the short term history of specific technologies, can provide a context to help us better understand our present situations, and inform our future choices.
Using an 1976 book on writing better COBOL as a guide, this talk explores the ways ""new"" ideas are old, how ""bad"" ideas were good (in their context), and how we might use those insights to better navigate a tech zeitgeist that insists on throwing out the old again and again and again."
Code4Bib&#58; Using APIs to identify publications that analyze research data held in a large social science domain repository,"The Inter-university Consortium for Political and Social Research (ICPSR) maintains the ICPSR Bibliography of Data-related Literature. The Bibliography, created in 1999, catalogs the usage of ICPSR data in research and hosts over 110,000 citations. The Bibliography is continually updated by a group of five information professionals who identify instances of ICPSR data usage in various publications. The process of discovering possible publications for inclusion in the Bibliography is quite labor-intensive due to the lack of data citation consistency. In an effort to increase discovery and decrease human intervention in searching, the Bibliography team has incorporated the use of indexer APIs to mine the literature for ICPSR data use. This poster will outline the challenges in identifying data usage in the literature, demonstrate the API code and workflow, and discuss possibilities for future enhancements to automated methods of finding data-related publications."
Automating FAST subject heading assignment in the repository - some initial challenges,"In this experiment with Python and FAST subject headings, an undergraduate student at Columbia University developed a program to take PDF inputs, extract the OCR, find frequent terms, and query them against FAST subject headings. Dissertations from the 1950s were used as the test set of documents. The resulting matches are exported from the program as a CSV which can be utilized in cataloging workflows within Academic Commons, the Columbia University institutional repository.

While the suggested FAST heading results were sometimes quirky, the project was very useful in mapping out the mechanics of how computer-mediated subject heading assignment might look in the near future. It was also useful in highlighting the continued need for human review and intervention in cataloging."
GenAI4Lib&#58; A Practical Introduction to Large Language Models,"The developers from the Digital Products & Data Curation workgroup at Northwestern University Libraries are proposing an immersive, full-day workshop designed to introduce and deepen your understanding of Large Language Models (LLMs) in the context of library services and operations. In “GenAI4Lib,” participants will learn from practitioners with firsthand experience creating, configuring, and deploying LLM-centric applications on the web.

- Real World Insights: Gain “behind-the-scenes” access to Northwestern University Library's
Digital Collections chat prototype to explore LLM content generation using state-of-the-art
commercial models
- Comprehensive Overview: Understand the current capabilities of generative AI models,
both open-source and commercial, and develop an intuition for the numerous factors that
can influence LLM output
- Direct Interaction: Learn how to use LLMs to generate content locally with open-source
models
- Innovative Techniques: Discover how Retrieval Augmented Generation (RAG) can ground
LLMs in institutional data, addressing issues like ""hallucination"""
Automating Metadata Hygiene to Improve Economic Research Discoverability,"Fed in Print is an application indexing papers, publications, and speeches from twelve Federal Reserve Banks and the Board of Governors. By presenting metadata to larger discovery services, including Research Papers in Economics (RePEc), Fed in Print serves as a discoverability driver for Federal Reserve System research. As a service indexing roughly fifty thousand resources to date, link rot is a stubborn problem for the application; content migrations cause links to break. This talk will report on a long-term project in progress, revealing tools and considerations that can be helpful in designing similar projects. Utilizing the recently deployed Fed in Print API, more than fifty thousand file URLs indexed in the application were tested for 404 (“resource not found”) errors and other discoverability problems. The broken URLs have been divided among collaborators throughout the Federal Reserve System. This talk will focus not only on the tools involved, but also project complications and benefits. Tools include Python, Postman, Excel, and Teams. Complications include coding challenges and individual capacity limitations. Benefits other than greater discoverability include API bug detection and, hopefully, initiation of proactive conversations between librarians and content professionals responsible for publishing material indexed in the application before future migrations."
Lessons learned: How to get traction with AI and start building,"Getting a grip on AI takes time, but after iterating through our first AI project, a method of comparing MARC records, we’ve gained enough skills and perspective to begin leveraging AI as a potential tool in our technology stack. There are many paths for incorporating AI, building a model from scratch, fine-tuning an existing model, or using a large language model through an API. However, we’ve learned through success and failure that there is a lot to consider before jumping into a solution. In this talk we’ll discuss how we’ve gotten up to speed, changed the way we collaborate, and the questions we now ask ourselves first when we consider using AI for a problem. We’ll talk about our current process for working with AI, and wrap up by sharing our ongoing AI work."
What You Can Do With Word Counts&#58; Learning the Basics of Text Analysis with Constellate,"Constellate offers a comprehensive and easily accessible program that helps your community develop text analysis and data mining competencies, along with related skills, through multiple modes of learning and hands-on application with tens of millions of texts from JSTOR, Portico, and other open source providers. In this workshop, Nathan Kelber (Constellate Education Manager) and Matt Lincoln (Senior Software Engineer) will introduce Constellate and discuss its teaching and learning focus. We will then preview upcoming new features, including new tools for doing deep dives into individual documents. In the latter half of the session we will demonstrate some text analysis with Python in the Constellate Lab and discuss how to use your own Jupyter notebooks."
kwalk: a simple program to crosswalk metadata for repository uploads,"Kwalk is a program that lets us write a simple crosswalk that we can apply to each batch of metadata as we receive it and have multiple crosswalks for multiple projects as we work on them in an intermixed fashion. The program allows us to apply special functions to modify date formats, combine literal and field name text, generate uniform upload URLs, and much more.

University of Chicago’s Center for Digital Scholarship has been utilizing this program to better edit metadata for batch upload to Knowledge@UChicago. There are plans to share this software in the future as it is platform agnostic and has a potential wide range of use cases."
Engaging in peer-to-peer teaching and learning: Students as Collaborators with the Library,"This poster presentation showcases student-initiated projects related to coding in libraries. The authors share their perspectives on creating supportive environments that empower students to pursue personal projects while contributing to the library’s mission. The presentation includes examples of peer-to-peer teaching and learning from multiple institutions that help build technical and coding fluency in students. The poster provides strategies for building an atmosphere that encourages student engagement, which viewers can use to implement student programs at their institution."
Building an open source single-search discovery implementation,"Lehigh University implemented and user tested a new single-search discovery experience this year, with the goal to simplify our library homepage with a single search box and a (default) results page that brings together resources from all of our public-facing platforms.  We prototyped new functionality and integrations on the VuFind platform, and A/B tested variations with our stakeholders to find a UX that worked.  New features we developed and contributed to the open source community include database recommendations that pull from both the article results and the A-Z list, a suggested librarian based on bibliographic call number ranges, and various UX improvements to VuFind’s existing combined search.  Data sources include open source platforms – FOLIO, ReShare, Islandora – as well as EDS, EBSCO Publication Finder and LibGuides A-Z Databases.  The poster will show the various integrations and UX improvements, including those that came from testing, and what we have planned next."
Designing Digital Directives,"The Permanent Legacy Foundation has implemented the first iteration of a legacy planning tool in our preservation-focused cloud storage platform. This tool allows our users to leave instructions to pass on their archive to a designated steward after their death or incapacitation. We see these instructions (directives) as a key part of our mission to preserve the digital legacy of all people. In this poster, we will share our understandings of digital legacy and highlight some foundational design principles for developing digital directives practices and/or mechanisms. We will examine our approach to building a tool that supports legacy planning on Permanent.org, in particular the user feedback that we sought and learned from along the way, and describe the schema and administrative settings we’ve created to support this feature in our platform. Finally, the poster will engage with challenges facing this existing system and our plans for future work."
Institution repository collection development with web scraping,"Many institutions report low rates of self-archiving with their institutional repositories, requiring repository managers to actively seek content to expand collections. This presentation discusses a method for collecting articles and metadata from open repositories using Beautiful Soup and Selenium as web scraping tools. Thousands of articles and corresponding descriptive metadata were quickly and easily added to an IR using this method, increasing visibility of research and engagement with the IR."
Exploring the Flip Side of Explainable AI:  Unexplained Expert Systems and Cultural Acceptance for the Unexplained,"Current buzz about artificial intelligence tends to present the excitement as coming from technological advances like speed to allow a real time conversation and better quality responses from chatbots and generative artificial intelligence.  This talk explores how cultural expectations, rather than technology, may be what recently reached a tipping point to put artificial intelligence into the public spotlight.  From a non-technological perspective, explainability is a major difference between machine learning and traditional software which encoded and applied logical rules.  Popular acceptance and embrace of machine learning requires a comfort level with not having an explanation for why the software does what it does.  This talk explores how past developments in how we interact with traditional software accustomed the general public to not getting an explanation, even for very explainable software.  Technology tools like skip logic in forms prevent us seeing the big picture within a logical system.  Cloud computing primed us to expect updates over time and accept constant changes outside our control to tools that we use daily.  When even imminently explainable software is not explained, explainability no longer matters.  This opens the way for the public to embrace machine learning."
M-PABI&#58; A Database Model for the Extended Specimens of a Pathogen Biorepository,"Natural history collections serve as vital infrastructure for monitoring and combating emerging zoonotic diseases by documenting wildlife-pathogen interactions through time and space. Flexible and searchable databases are essential for storing and facilitating access to the object data, preparation types, and collection occurrences that document the full story of the extended specimens within these collections. This poster presents the development of a Specify database and Darwin Core metadata schema for the Michigan Pathogen Biorepository (M-PABI), addressing this need for accommodating various specimen data types and complex host-pathogen relationships. The challenges and solutions discussed provide insights into creating flexible data architectures applicable to interconnected organismal collections. Holistic databases such as the one detailed here can enable interdisciplinary collaboration, open science, and enhanced pathogen surveillance, ultimately contributing to zoonotic resilience and biodiversity research."
"Creating Accessible Documents As Non-Authors: Tools, Strategies, & The False Idol of AI","Documents are only accessible if all users can examine their contents, yet many of the ""openly accessible” articles, books, and documents at libraries are often not compatible with text-to-speech readers, preventing low-vision users from accessing their information. The barrier to uploading accessible material is high, as making these documents accessible for low-vision users can be an arduous task for those who are not authors of the material, and there is little recognition when it is performed by either the author or library staff. As a result, accessibility work is often done haphazardly, leaving large swathes of online library content inaccessible for low-vision users. Grappling with this as I worked on projects with my University’s Institutional Repository, I performed a research project to examine our accessibility workflows and update them to be more rigorous and time efficient. In a talk targeted at fellow time-pressed accessibility advocates, I will discuss what the research tells us makes documents accessible, how to triage accessibility requirements, and the available tools and strategies for making documents accessible efficiently, including several emerging AI based tools which show tremendous promise."
Enhancing Cataloging of Electronic Government Documents with Programming and OpenAI,"The advent of AI products like OpenAI ChatGPT has surged into the learning and library scenes, prompting us to contemplate how to harness these advances to enhance our work and envision the future of our endeavors. Especially within the domain of electronic resources and born-digital materials, developers have utilized their programming skills to extract essential resource descriptors from source files. The fusion of these programming skills with AI technologies empowers metadata and cataloging professionals to redefine and automate their workflows to a certain extent, resulting in increased efficiency.

This presentation aims to demonstrate the experiments we did, utilizing Python and OpenAI APIs to enhance the efficiency of cataloging electronic government publications in the MARC format. We will not only present our ongoing research and discuss the challenges we’ve met but also engage in an insightful dialogue concerning the potential impact of these technologies on the evolution of cataloging practices in the future."
Leveraging AI Tools for Automating Metadata Extraction,"With an increase in the accessibility of Generative AI and Large Language Models (LLM) we now have the ability to apply these tools against a variety of materials, including images, text, and audio.

The UCLA Library is experimenting with applying AI/ML tools against digital materials that would typically require significant human intervention to extract relevant metadata. Examples include text extraction via OCR, interview transcription, and metadata record generation from local digital library objects.

In this discussion we outline our experience applying some of the more common AI tools against a collection of digital library material with complex layouts, with the aim of building a foundation for the creation of a partially or totally automated metadata pipeline.

Topics and tools discussed:

* Optical Character Recognition (OCR) tools
* Named Entity Recognition and related trained models
* Applying LLM tools such as ChatGPT and Bard to extract metadata"
Unique Exhibits and Minimal Infrastructure&#58; Creating Static Digital Collections With CollectionBuilder and GitHub Pages,"Looking for an agile, lightweight approach to developing digital collections and scholarship projects? Participants in this workshop will use the static web framework CollectionBuilder to create a digital collection website that encourages browsing and contextualizing items through timelines, maps, and word cloud visualizations.

The high cost and IT requirements of digital collection platforms are often a barrier to creating and customizing new digital projects. CollectionBuilder is optimized for iterative development and simple hosting solutions, empowering users to take greater ownership over their digital projects and enabling sustainable custom sites on minimal infrastructure. Utilizing well-structured metadata and a directory of digital files, CollectionBuilder employs a Jekyll static web generator to create a website for visualizing and accessing the collection.

Participants will explore a variety of development environments (including GitHub Codespaces as well as Ruby and Jekyll for local development) as they walk through the steps of creating a collection, touching on concepts such as generating derivative images of their digital objects, configuring their collection using YAML and CSV files, and adding interpretive content. Workshop activities will also include setting up GitHub Pages to serve a CollectionBuilder site as well as advanced customization topics such as adding compound objects, customizing CSS, and navigating hosting options."
Build a recomendation engine using AI,"You may have heard of AI tools like ChatGPT and OpenAI, but have you put them to work for your library? This workshop is a gentle introduction to AI by building a recommendation system for library materials.

Our existing online search has helped our users find materials, but they've really only been doing text matching. Now, we can use large language models to do text analysis and AI tools to build recommendations in a different way. In this workshop you can create a simple application for helping patrons find materials they are interested in, but didn't know existed. Along the way we'll build skills to implement similarity searches for other data sets and learn about some useful tools for working with large language models."
Encoding Reparative Description: Developing Tools to Analyze Problematic Finding Aids,"Over the past few years, more archives and archivists have been working on enhanced description projects that can address past inequities, erasure, or incorrect representations in description. This work has been variously described as reparative description, mindful description, conscious editing, and by other names. Whatever it is called, while this work is typically grounded in slow, relationship-based work within and outside of archives, the identification of problems and addressing of changes can benefit from computational approaches. Stemming from the “ReConnect/ReCollect” project at University of Michigan, which has surveyed the extent and legacy of colonial collections extracted from the Philippines since the late nineteenth century, we report on work to analyze more than two hundred finding aids with the development of Python-based analysis tools. We are not advocating for technical solutions to fundamental problems, but we are interested in showing how such automation can help to expand the project of reparative description. Our presentation will report on work by students and faculty at the University of Michigan School of Information, collections managers and curators across the University, and systems managers at these various institutions worked together to aggregate finding aid metadata, analyze that descriptive information for potentially harmful, outdated, or problematic terminology and tone. We conclude with information about the code that we used, and the potential challenges and benefits of adopting automated approaches in the implementation of reparative description."
"Change is Afoot: What WCAG2.2, WCAG3, ..., WCAG9000? Mean for Libraries","As of October 2023, the Web Content Accessibility Guidelines (WCAG) were officially updated to version 2.2. This new W3C recommendation introduces nine new success criteria (ranging from A to AAA) and a handful edits to existing criteria. This talk will discuss the potential implications of WCAG 2.2 for our library web content and provide potential fixes if necessary. Moreover, I will discuss how new WCAG recommendations are crafted, including some preliminary discussion and cautions regarding WCAG3 and the future of the web accessibility."
Cleaning metadata in bulk through batch editing,"Collections of any significant size or age often include data that is not consistent across records. Frequently this data can be normalized by applying a large number of small but similar changes across groups of records. Tools that support bulk changes to groups of records can make this kind of collection maintenance practical without a large investment of time in manually re-cataloging individual records. The Metavus digital collections platform includes a BatchEdit plugin explicitly designed to support this sort of workflow. Records to modify are selected by adding them to a folder, either individually or by adding all the results from a search. For controlled vocabularies, BatchEdit can clear a field entirely, clear specific terms when present, or add specific terms. For fields that contain textual data (e.g., Paragraph, URL, Email, etc), BatchEdit can prepend data, append data, and perform find/replace operations. These operations can be applied in combination to quickly normalize records to follow a consistent set of standards."
Rethinking Digital Asset Management at University of Texas with OCFL in Fedora 6,"We at University of Texas at Austin Libraries are leveraging the standards-based format, OCFL (via Fedora), to create modular and easily maintained digital repositories. To address pain points maintaining, enhancing, and especially migrating our services over the years, we are rethinking how we build them from the ground up. We are using standards-based formats like OCFL to create modular, API-driven repositories that can be upgraded or replaced without compromising the data preserved within. Additionally, over the past few years, we have transitioned to Kubernetes and this has had a large impact on how we look at our app architecture as a whole. We’d like to share our experiences and outcomes reconstructing a repository with this mindset last year, and how we are applying the lessons learned to our current and future projects."
Common Pitfalls of Project Management In Academic Libraries,"Few libraries have a project management office that can carry out their projects in a systematic matter.  Developers, supervisors and technicians are often called upon to conduct their own project management while also doing much of the project work themselves.  In this presentation I will cherry-pick the choicest bits of wisdom from the PMBOK Guide and tailor them for library scenarios where work units are divided into functional departments and project managers serve multiple roles.  I will structure this talk around common issues that arise with library projects and show where project management best practices can be applied to remedy those issues.  Attendees will gain knowledge for successfully planning projects and for controlling work over the course of a project.   For this presentation I will be drawing on the PMI 99-001 standards for project management as well as mixing in a few of my own insights."
Python{4}Lib,"Python{4}Lib is an informal discussion group which meets biweekly over Zoom. The first hour of this workshop will be one of these informal discussions; a lightly guided chat about all things python in GLAM. For the rest of the session, we will focus on a variety of topics related to the python language with an eye towards improving our understanding of concepts as well as learning new things. Topics will adapt based on the attendees' interests, but may include: pymarc, pandas, spaCy, pytest, csvkit, Jupyter Notebooks, typing, common python patterns, powerful but underutilized components in the standard library, (relatively) new python 3 features, asynchronous code, code editor configuration (linting, pylance), and dependency management (virtualenv, poetry, pipenv).

In general, the session is intended for people with some experience programming in python. Novice to intermediate programmers should expect to learn new techniques but also see some familiar code. Beginners may feel a bit lost during conversations, but there will be beginner exercises and instructor assistance available."
An Archivematica-Powered Digital Preservation System for the General Public,"The Permanent Legacy Foundation provides endowment-backed archival cloud storage directly to private individuals and small organizations. We are currently in the process of rebuilding our file processing pipeline. To do so, we will leverage a fully automated Archivematica pipeline in our backend to both improve performance and better adhere to archival best practices. We are exploring different deployment approaches for the best balance of cost, performance, maintainability, and scalability. This poster will cover how we chose Archivematica, our design for integrating it into our processing pipeline, how we might iterate on that design in the future, challenges that have arisen during implementation, and how we plan to present the institutional-grade preservation available through Archivematica to an audience of mostly non-specialist users."
Where should I study? How to help students discover and access the right library study spaces for their needs.,"The University of Michigan Library launched their shiny new website in 2018, and while it was an incredibly successful redesign, there were still a few finishing touches missing. Many well-respected universities, including The University of Cambridge and Harvard University, have implemented a searchable and filterable study space finder to help students on campus find places to study that fit their unique needs, and we wanted to make one too! We designed a prototype, using our design system, of our own study space finder based on an evaluation of similar systems. Our team conducted two rounds of user interviews with 25 U-M students to first assess user needs and generate requirements for our design, and then to evaluate our design with an interactive prototype to ensure it would meet the needs of U-M students. When developed and launched in the near future, we expect this system will be a great resource for the U-M community!"
"One Team, Multiple Initiatives: Project Management and Collaboration Across a Library Team","How does a team balance multiple projects and conflicting priorities? Using the New York Public Library (NYPL)’s eReading team as a case study, this presentation will walk attendees through methods of collaboration and resource sharing from a project management perspective. NYPL has developed multiple eReading apps that are used for different sets of patrons: SimplyE, where patrons from participating libraries can check out and read eBooks on their devices, and Open eBooks, where students at Title I schools can access free eBooks. On our eReading team, developers contribute to both projects, while specialized project and product managers lead each initiative. We will discuss best practices and lessons learned through our project management methods."
Wrangling the Past: Using Python to Prepare Legacy Digital Collections and ETDs for Preservation,"This presentation will explore the ways in which Python scripts, both large and small, are effective tools for normalizing large amounts of data that make up disparate digital collections and preparing them for digital preservation. After setting up a digital preservation program, our institution was faced with a backlog of 33 digital collections, totaling 3.7 TB of data that was generated over the course of 15 years, as well as 17 years’ worth of electronic theses and dissertations (ETDs). We set out to appraise, organize, and package each collection for ingest into our digital preservation repositories with the aid of python scripts. The presenter will detail how digital preservation principles dictated scripting decisions, as well as considerations for implementing similar strategies at other institutions, including building workflows and troubleshooting scripts. The presenter will end with lessons learned from the completion of this large-scale project."
Integrating Digital Library Platforms: The DIY Approach,"What's more frustrating than setting up a library system? Setting up TWO library systems! Especially when they won't talk to each other.

When the Video Game History Foundation started building their library, they had trouble getting their archives management platform (ArchivesSpace) to play nicely with their digital repository (Preservica). While Preservica has built-in integration features for ArchivesSpace, the VGHF team wanted more flexibility and functionality than what the official tools could offer. Using the APIs for both platforms, they developed their own CLI program that gives them more control over how they synchronize metadata.

In this talk, you'll hear from the odd couple of VGHF's library team — an academic librarian and a platform engineer — about how they combined their skillsets to build a tool that worked better than the off-the-shelf solution. Will the engineer learn how to do a MODS crosswalk? Will the librarian learn how to debug Python?"
Multi-Modal Machine Learning to Enhance the Accessibility of Natural History Collections,"University libraries and museums hold vast collections of curated image data that document the natural and social world. While these data are, in theory, accessible to professionals and the general public, in practice, searching archival collections requires technical knowledge and relies on precise scientific terminology, which can be a barrier to its accessibility and broad appeal. Our project aims to enhance the accessibility of natural history collections data through artificial intelligence. We evaluate different “multi-modal” machine learning frameworks – combining insights from natural language processing and computer vision – and compare their performance to human subjects. We then develop a system that unifies collections images and both novice-and expert-level natural language descriptions. We apply our system to museum collections at the Florida Museum of Natural History to assess its effectiveness for querying existing databases. Preliminary findings suggest that larger models may not consistently outperform their smaller counterparts in certain image-to-text tasks. The workflow is transferable to broader text- and image-centric museum and library collections, laying the groundwork for custom natural-language search tools tailored to unique collections."
"Designing a Flexible, Standards-Adherent Date Field for Novices and Professionals","Permanent.org is a cloud-based digital preservation system aimed towards the general public. Though Permanent.org has a date field within its metadata for members to edit the creation date of the files they upload, there are many pain points that result in a somewhat-negative user experience for our members. Permanent strives to create a date field experience that allows for the many use cases of the platform. Balancing the needs of multiple different audiences with dates can be a challenge. Some Permanent.org members are professional archivists who prefer to use specific metadata standards, while a novice member may not know about or care to use a specific standard. People who upload born-digital material may care about the time something was created, whereas a family historian may only know a year or month for an old family photograph. This poster will explore our current progress on redesigning our date field to be flexible enough for our members but still adhere to archival and software best practices and standards for dates."
Teaching coding online to full-time library workers,"My organization developed and ran a free 9-week online class titled ""Fundamentals of Health Data Science"" aimed at an audience of medical librarians. The course covers an introduction to Python in the context of working with a health-related dataset. The course was mostly asynchronous, with a group of 70 learners working on the same topics week-by-week, but we also hired librarians with Python experience to meet with learners and help them with the class. We think this format addresses some of the traditional challenges of completely asynchronous learning (isolation, lack of feedback) and completely synchronous classes (scheduling). This talk would present some background information on the class, the instructional approach we took, the challenges and successes we had, and the outcomes/feedback from class learners and helpers. This class will be repeated in the future, and presenting on it could help us receive more feedback on how to improve it going forward."
"From PDF Coordinates to GeoJSON&#58; Transforming the DTE Aerial Collection, One Map at a Time","In 2022, Wayne State University Libraries began migrating 50,000 digital collection items to OCLC’s CONTENTdm, moving from an in-house system unable to ingest new items since 2018 due to eroding support structures. However, switching to a vendor system meant that we had to finally modernize one of our most used (and most outdated) collections, the map-based DTE Aerial Photo Collection. Despite its popularity with researchers, DTE Aerial relied on increasingly obsolete browser technology, and after 20 years of stagnation was only accessible using Firefox. Using a workflow developed in 2019 by students at the University of Michigan’s School of Information, we migrated the high-volume collection of PDFs from a local server to Google Cloud Platform for ease of access, reviewed and adapted the legacy code by updating packages (PyPDF2), and implemented a system to batch process the PDFs and troubleshoot errors. This updated workflow allowed us to generate JPGs and transform the geographic coordinates from points on a PDF map to GeoJSON, and finally into usable metadata. From there, we were able to ingest the aerial images into CONTENTdm, create usable maps with Leaflet, and ultimately ensure the viability of this important collection for years to come."
Digital Forensics for Libraries and Archives&#58; Introduction to Using BitCurator,"BitCurator (BC) is a digital curation environment geared toward librarians and archivists that combines a set of digital forensics tools in the Linux-based Ubuntu operating system. The BC environment includes a suite of open source digital forensics and data analysis tools to help collecting institutions process born-digital materials. It can be run as a main operating system on a single workstation, or it can be run in a virtualized environment (that is, the entire system is running by itself inside another program on your computer).

Assuming no prior knowledge, this workshop will first provide an overview of the environment. We will then focus on an exercise to create disk images of and transfer data from legacy media, including Zip disks and 3.5” floppy disks before walking through some of the tools available on the BC environment for analyzing them, including capture of basic system characteristics and metadata, as well as scanning for potentially sensitive information. Through a combination of hands-on work and demos, participants will gain experience with tools such as Brunnhilde (a reporting tool for directories and disk images), bulk_extractor (scan for credit card numbers, emails, etc.), and fiwalk (print filesystem statistics). The workshop will then turn to a discussion of how to build workflows that fit your institution’s needs, open discussion of digital preservation practices, and continued work on your own using sample removable media."
Fail4Lib 2024,"Although failure is part and parcel of our professional experience, we often go out of the way to avoid talking about it. When we see failure approaching, we distance ourselves, avert our eyes, or -- if we're in its path -- brace for the worst. But failure has intrinsic value and is an essential step on the path to professional and organizational success. And since it’s inevitable, we should learn how to look back on our failures to derive value from them, and how to look ahead so that our past failures can inform our future successes.

Fail4Lib is the perennial Code4Lib preconference dedicated to discussing and coming to terms with the failures that we all encounter in our work. It is a safe space for us to explore failure, to talk about our own experiences with failure, and to encourage enlightened risk taking. The goal of Fail4Lib is for participants – and their organizations – to get better at failing gracefully, so that when we do fail, we do so in a way that moves us forward.

Both first-timers and Fail4Lib veterans are welcome!"
Beyond DSpace&#58; DSpace-CRIS & DSpace-GLAM as extended datamodels,"Digital repositories play a pivotal role in managing and disseminating scholarly and cultural content, with DSpace emerging as a prominent open-source platform. However, the ecosystem of DSpace has evolved with specialized extensions developed by 4Science called DSpace-CRIS and DSpace-GLAM, each tailored to distinct domains. DSpace serves as the foundation, offering core repository functionalities for academic institutions. DSpace-CRIS extends this by integrating Current Research Information Systems, facilitating comprehensive research management. Conversely, DSpace-GLAM caters to galleries, libraries, archives, and museums, focusing on the preservation and discovery of cultural heritage collections.

This poster session provides a comparative analysis of DSpace, and its extensions DSpace-CRIS, and DSpace-GLAM, elucidating their unique features and functionalities. While all three share DSpace's core capabilities, they diverge in their extensions. DSpace-CRIS empowers institutions with advanced research management tools, enabling the aggregation and showcasing of scholarly activities. In contrast, DSpace-GLAM addresses the specialized needs of cultural heritage institutions, facilitating the digitization and dissemination of diverse heritage collections.

Understanding these distinctions enables institutions to align their repository platforms with their specific objectives, whether in academia or cultural heritage preservation. By exploring the diversity within DSpace, stakeholders can make informed decisions to optimize their digital repository infrastructure."
Local Discovery Layers,"Some libraries have built their own discovery layers to interface with their Integrated Library System (ILS), Library Management System (LMS), and/or Library Services Platform (LSP). If you count yourself among them, let’s get together and talk about what we’ve learned about using local search indexes (i.e., Solr), using the APIs supplied by the ILS/LMS/LSP vendors, connecting to other sources of data, working with large sets of library data, and building usable and accessible front ends. This half-day session will be organized unconference-style. While we have some possible topics to start the conversation, attendees will decide on topics together and then discuss as a group or form breakouts for niche conversations. We’ll bring summaries of discussions back to the whole group and make plans for future interactions and action as appropriate.

Possible topics as seeds for discussion include:

- What new features or modifications have you been exploring?
- What are the particular pain points, and what could we collectively do to encourage vendors to help solve them?
- What role do UX and Accessibility play in your application development?

Who should join this workshop? Application developers, front-end developers, UI/UX designers, and project managers or other roles in building, planning, or maintaining these systems."
"Secrets and Lies: Exploring and Exploiting APIs That Are Undocumented, Underdocumented, or Misdocumented","Sometimes you find a service or website that serves up information that you want to use, and you want to automate your calls to it, but there's no documentation, or whatever documentation exists is less than helpful or even wrong. This presentation will discuss and demonstrate some techniques and tools for experimenting with calling APIs to get the results you want. Tools mentioned may include curl, wget, jq, browser developer tools, Postman. Demonstration may be prerecorded for reliability and efficiency."
Automating File Organization by Leveraging Patterns with Python,"When faced with organizing a backlog of digital material totaling 144,000 files, it became clear that manual file organization was not a tenable option. Patterns in the files quickly arose and python became a catch-all solution to automating file organization that leveraged those patterns. This poster will highlight the ways in which python can automate various tasks, such as bulk renaming, breaking up larger directories into individual objects, and sorting files into new folders based on a given pattern. It will also address best practices in testing scripts before using them on your data."
Programmatically Processing OCLC Cross Reference Files,"The University of Michigan Library needed a way to process the thousands of lines in Bibliographic Cross Reference files from OCLC produced as part of the data sync process. The Bibliographic Cross Reference files contain the local system control number (LSN) and the new OCLC control number (OCN) from matched records in WorldCat. An application was needed to go through each line, process the data, create reports, and, if needed, edit the bibliographic record in the repository. There was also a desire to update the bibliographic record with supplementary information from WorldCat not included in the Bibliographic Cross Reference files in some cases. This poster illustrates an application built by the Automation, Indexing, and Metadata department. Programming in Ruby, collaborating in GitHub, containerizing with Docker, and connecting with the Alma API and the WorldCat Search API, this application makes processing the Bibliographic Cross Reference files a snap."
Make Time for Design: Integrating Design Ops Creates Big Value for Small Teams,"In higher education IT, there are more developers than dedicated designers on a team. Oftentimes, there are no designers allocated to the ongoing work of a digital product. Designers and design work feels siloed and mysterious. In an environment that values continuous improvement and shipping work, how do we advocate and make time for design, UX, and accessibility without causing bottlenecks or frustration? This talk explores how investing in design ops and design systems can bring together design and engineering and creates a culture of confidence, collaboration and creativity with our small teams.

The U-M Library Design System team formed in 2017 with two members. We created the design system to reduce duplication effort and amplify our impact on the UIs we were building. But using a design system alone is not enough. In 2020, we began focusing on the bigger picture, investing in various design operations for our tech division. This “invisible” work has created a collaborative, design-led mindset in our product teams as we work together building accessible, usable digital applications. Everyone can do design with the right tools and systems."
We Can Build That: Custom Python Applets for Digital Preservation Processing,"When processing digitized and born digital collections many archivists and librarians are faced with two potential pathways 1) using open sources tools, many of which require intermediate to advanced technical skills or 2) using specialized software, most of which are not tailor made to digital preservation use cases. The Digital Scholarship Unit (DSU) at the University of Toronto, Mississauga Library faced this same dilemma when digitally preserving the EP Media Fonds, which is comprised of both analog videotape and born digital video. While digital video converters abound, neither out-of-the-box commercial nor open source solutions quite fit the bill. Software like Adobe Media Encoder was not capable of converting formats such as XDCam and alternatives like FFmpeg were too complex for some staff to utilize effectively. For this reason, the DSU sought to develop custom applets using Python that streamline the preservation process by providing a simple graphical user interface with a custom back-end that allowed novice users to utilize libraries like FFmpeg and Bagit to convert, create preservation and access copies, and package digital media for preservation purposes without requiring any knowledge of command line interfaces or video transcoding. This presentation will review the development process and how similar tools can be developed for use by student and non-technical staff in libraries and archives."
OCR and PDFs - the possibilities and limitations for accessibility,"In order to serve its patrons, NYPL is constantly finding new ways to make more content easily accessible through digitization. In order to expand on our research catalog and get the most out of digitization, we have started work on using Optical Character Recognition (OCR) on scanned books to not only provide digital copies, but to improve accessibility with tagged and searchable PDFs of our research materials. In this talk, we’ll explore the technical details of our new PDF creation pipeline, as well as both the possibilities and limitations we’ve encountered in using OCR data."
Automation of Subject Analysis in Collection Evaluations,"Every year we analyze an assortment of subject-based collections in order to make recommendations for program accreditation, enhancement purchases, and promotion. We typically start this process with a needs assessment and with analyzing collection-level trends. The final step of the evaluation process is to do a deep dive into each subject to examine a selection of raw and calculated measures (such as ILL requests, usage, recency, and the percent of Choice’s Outstanding Academic Titles list held) and assess whether the subject is strong, meets needs, or needs improvement. This process used to be completed manually, but with an average of 116 subjects per collection, this can be a very time-consuming, complex, and subjective process. In this presentation, we will discuss how we automated this process using a combination of SQL, Python, and regression to condense data about holdings, usage, ILL, and more into three scores for sufficiency, quality, and interest. We will also discuss how we use these scores to make recommendations that streamline the purchase process and enable targeted usage of our limited funding."
The Art and Science of Perfectly Tolerable Web Forms,"No one loves filling out web forms. Too often, they're annoying, frustrating, or downright diabolical. But web forms don't have to be this way! In this hands-on workshop, we'll cover how to improve forms like the “Contact Us” or “Petition a Fine” forms you might have on your institution’s website. We’ll cover practical design considerations, wording choices, and technical details that will help keep your website’s forms a painless experience for your users. Using plenty of good (and ghastly) examples, we’ll consider everything from titles to error messages to automated emails. We’ll also touch on how to collaborate productively with your colleagues when co-creating a form. I’m excited to share my passion for web forms with you and help you ensure that your users find your forms perfectly tolerable. Every form on a library's website can benefit from this workshop’s practical usability improvements!"
Everything could have been different: why computers are like that,"There are certain things we take for granted: bytes have eight bits, the internet uses packets. But these are decisions made by humans when other alternatives were on the table. In this talk, I’ll tell stories of some papers that were pivot points in the history of computing. How is the internet a response to nuclear war? How did a neuropsychologist and a tragic genius dream up neural nets? There will be an absolute banger of a bibliography for you to continue learning with."
Making the website of a cash-strapped academic library work&#58; lessons on LibGuides.,"The University of Illinois at Chicago Library has been gradually overhauling its website over the past few years.  This has happened in fits and starts, in the midst of reduced staffing, reduced budgets, the loss of support infrastructure, and burgeoning demands from administration.  How can a tiny team with big responsibilities keep it together?

To make it work, we have doubled down on existing systems to offload administration: shifting our self-hosted website to a University-supplied host, adopting a consortial discovery system, and leveraging more features of the Springshare systems to which we already subscribe.  This has saved money and bandwidth, sometimes at the cost of less control and a loss of features once taken for granted.  But it has nevertheless provided a foundation for stability, and a launchpad to reach toward the future.

This poster session would present a timeline of our decisions, a map of the systems adopted, and a description of how we have bent Springshare systems to our needs.  It will feature technical guidance, gotchas, and a wish list for an as-yet unrealized academic library content platform."
The Last Two Women Standing:  libtechwomen's CodeClub,"Late in the year 2014, a call went out on the libtechwomen mailing list proposing the formation of CodeClubs, with the idea that reading other people's code is an essential part of learning to be a good developer.

Two CodeClub groups of 4-5 librarian/library workers each had a weekly virtual meeting, where code, usually from GitHub, was read and analyzed.

 All these years later, two of us, a cataloger and a library software developer, are still meeting weekly over Zoom.  However, not only are we reading code, now we're writing it!

We will describe the work we are doing and discuss the benefits of the CodeClub approach."
"Streamlining Web Security&#58; Embracing Let's Encrypt SSL Certificates for Hassle-Free, Automated Renewals","In today's digital landscape, securing online communications is paramount. This poster examines the adoption of Let's Encrypt SSL certificates as a hassle-free solution for website encryption. Let's Encrypt offers numerous advantages, including automated renewal processes and reducing the manual intervention typically associated with certificate management. By leveraging scripts, administrators can ensure seamless updates before expiration, enhancing the security posture of their web assets while minimizing operational burdens. Moreover, Let's Encrypt is compatible with various web servers such as Apache2 and NGINX, and its installation and renewal can be effortlessly facilitated through automated scripts, streamlining the implementation process across different environments. This poster examines the benefits of Let's Encrypt SSL certificates and provides insights into their integration within existing library infrastructure, empowering organizations to fortify their online presence efficiently and effectively."
[Re]Thinking Digital Infrastructure: Centering Humans in Integrated Systems Work,"In this talk, librarians and archivists from an academic library will discuss establishing and cultivating a Digital Infrastructure Working Group (DIWG). The DIWG was born out of a need to create interoperability between metadata, preservation, discovery, and access systems for archival materials, and demanded a radical rethinking of prior organizational approaches to digital asset curation, including leveraging available APIs and vendor contracted system development. Learn how team members approached technological work by centering each other’s well-being and intentionally making visible the often-invisible human labor invested in developing and maintaining digital infrastructure. We will share how the DIWG challenged organizational silos, established cross-departmental communication and workflows, and cultivated a climate of professional support and human-centered care in a pandemic and post-pandemic workplace. Key takeaways include recommendations made to library administration for digital infrastructure curation moving forward."
Are We Donne Yet? The Decade Long Project of Converting an Annotated Bibliography from Print to Database,"“Do you think we could create a searchable database?"" Do questions like this fill you with dread? Or maybe excitement? They can and should do both! This superficially simple question led an inter-institutional team of librarians and scholars on a decade-long project of various stages to convert the four-volume John Donne: An Annotated Bibliography of Modern Criticism into a user-expandable, searchable database. The project started as a two-person project that was later integrated into a larger grant funded team. We will share our experience as an interdisciplinary team implementing a text-base digital humanities project that has become an open-access, fully searchable Drupal database that will soon be available to the public.  We will outline some of the challenges we faced taking this from print into electronic and the ways in which the digital format forced a kind of precision and standardization that is not common in humanities resources, and conversely, the challenges of designing a database that could accommodate the ambiguities and vagaries of the materials humanities scholars work with. We'll also highlight considerations from throughout the project, from early, fundamental questions regarding database and workflow design, to more recent pragmatic problems such as standardizing author, subject, and work indexes from a body of work that spans 4 decades of evolving scholarship."
Whispering at any volume: Scalable speech recognition for all,"The American Archive of Public Broadcasting (AAPB) preserves and hosts over 150k audiovisual resources, with over 100k records available publicly. A collaboration between the Library of Congress and GBH (WGBH Boston), the collection spans over 100 years of media history.

Accurate transcripts remain critical for content discovery and accessibility, especially with such an extensive catalog. In September 2022, OpenAI released Whisper, a speech recognition model that revolutionized our transcription capabilities.

This presentation compares Whisper models against existing transcription approaches and demonstrates major improvements in accuracy and speed, while significantly reducing computing power. We examine test results across models and compare them to the popular Kaldi speech recognition toolkit, noting significant improvements in operational cost, especially with GPUs. In addition, we highlight Whisper’s multi-language capabilities and discuss common errors and biases.

Throughout, we showcase real-world examples along with source code for operating at multiple scales: starting from a single laptop, horizontally scaling to a group of machines, and vertically scaling to a Kubernetes cluster with GPU nodes. Here, we feature demos from “Chowda”, our new open-source web application for processing large batches of media through Whisper and additional workflow pipelines."
Community Source Software: Complex collaborations as told by two Program Managers,"The decision to implement open, community-supported technologies can bring with it complexities and efforts not always experienced by the use of proprietary solutions. “Open Source”, by definition, can also lead to confusion and misunderstanding of the true cost of implementation, as well as underestimation of the importance of collaboration and advocacy.

Program Managers are in a unique position to help build collaborative bridges as a means of uniting users, supplying actionable use cases, and fostering open dialogue. In this talk, we will discuss commonalities and differences in our roles and how our respective organizations must collaborate, both internally and cross-organizationally, in an attempt to meet the needs of multiple technical and user communities in the digital repository realm.

As Program Managers of both an open-source, community-supported software, and an academic library’s digital repository, which uses said software, presenters will highlight and discuss their perspectives, methodologies and their on-going work to attempt to solve issues faced by many others within their own communities. Through exploratory work to identify barriers to adoption, migration and paying technical debt, we will discuss common goals and challenges to sustaining preservation repository technology across complex open frameworks."
Modern Alchemy&#58; Hands-On with the Distant Reader,"In this hands-on workshop participants will first learn how to use a Web-based system called the Distant Reader to transform sets of unstructured data (like bunches o' journal articles) into structured data, affectionally called ""study carrels"". These data sets are amenable to analysis by both people as well as computers. Second, participants will learn how to use both command-line tools as well as GUI applications (like AntConc, OpenRefine, or Gephi) to analyze the data sets. This analysis includes things such as feature extraction, concordancing, topic modeling, full text indexing, semantic indexing, network analysis, etc. In the end, participants will learn of an additional ways to turn data into information -- modern alchemy."
